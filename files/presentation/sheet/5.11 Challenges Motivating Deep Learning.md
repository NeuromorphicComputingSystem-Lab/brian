# Challenges Motivating Deep Learning


전통적인 머신러닝에서 일반화를 달성하는 데 사용된 매커니즘이 고차원 공간에서 복잡한 함수를 학습하기에 어떻게 불충분한지에 대해 설명한다. 또한 이러한 공간은 종종 높은 계산 비용을 필요로 한다.

딥러닝은 이러한 문제들을 극복하기 위해 설계되었다.


## Ther Curse of Dimensionality
- 수학적 공간 차원(=변수 개수)이 늘어나면서 ,문제 계산법이 지수적으로 커지는 상황
- 차원이 높아질수록 데이터 사이의 거리가 멀어지고, 빈 공간이 증가하는 공간의 성김 현상(sparsity)을 보임
	- k-NN 분류 알고리즘에서 흔하게 발생하는 문제


- 10^1
- 10^2
- 10^3

### 어려움
- 대표적으로 통계적인 어려움이 있다. x의 possible configurations의 수가 훈련 샘플보다 훨씬 많기 때문에 통계적 문제가 발생한다.
- 이 문제를 이해하기 위해서 입력 공간이 그림과 같이 격자로 구성되어 있다고 생각해보자. 저차원 공간은 데이터에 의해 대부분 차지되는 적은 숫자의 격자 셀로 표현할 수 있다.
- 새로운 데이터 점들로 일반화시킬 때에는 새 입력값과 동일한 셀에 있는 학습 데이터를 확인할 수 있다.
	- 예를 들어 어떤 점 x의 확률 밀도를 추정하는 경우, x와 동일한 단위 부피 셀에 있는 학습 데이터 수를 총 학습 데이터 수로 나누면 된다.
	- 만약 분류를 한다면, 동일한 셀에 있는 학습 셋에서 가장 많은 클래스를 반환하면 된다.
	- 회귀를 할 경우, 해당 셀에서 관찰된 값의 평균을 낼 수 있다.
- 그러나 만약 본 적이 없는 데이터는 어떻게 해야할까? 고차원 공간에서 configurations의 수는 많기 때문에 일반적인 격자 셀은 학습 데이터셋이 없다. 따라서 새로운 configuration에 대해서는 의미있는 결과를 말할 수 있다. 기존의 많은 머신러닝 알고리즘은 단순히 새로운 지점의 출력값이 가장 가까운 학습 데이터 지점에서의 출력값과 거의 동일해야한다고 가정한다.


### 차원의 저주 피하기

#### 차원을 줄이는 알고리즘 사용
- PCA(Principal Component Analysis)
- LAD(Linear Discriminant Analysis)
- LLE(Locally Linear Embedding)
- MDS(Multidimensional Scaling)


## Local Constancy and Smoothness Regularization
In order to generalize well, machine learning algorithms need to be guided by prior beliefs about what kind of function they should learn.

머신러닝 알고리즘은 선택하고 학습하는 함수들의 prior belief에 의해서 특성이 달라진다. 가장 흔하게 사용되는 prior는 smoothness, local constancy다.

이 prior는 sample들이 살짝씩 달라지더라도 학습 결과는 크게 변하지 않는다. 이것이 지켜지지 않으면, 고차원의 데이터 공간을 우리가 가진 데이터들의 분포로 잘 설명하지 못하고, 따라서 분포를 사용하는 어떠한 테스크도 최적화하기 어렵다.

f*(x) ~= f*(x+E)

약간의 E의 차이가 있어도, 결과는 x와 유사해야 한다는 prior다. 이러한 prior를 표현하는 implicit(절대적인), explicit(명백한) 방법은 많이 있다. local constancy의 극단적인 예시는 k-NN이다. knn에서 test data (x)는 k개의 근처 학습 데이터(x + E)들과 같은 클래스로 구분된다. (f*(x) ~= f*(x+E)).

kernel machine에서는 test data와 근처 학습 데이터를 interpolate(보간)한다.

local kernel k(u, v)는 두 벡터의 거리가 가까울수록 커지고 벡터의 거리가 멀수록 작아진다. olcal kernel은 템플릿 매칭을 하는 similarity function으로도 볼 수 있다. decision tree는 파라미터를 기준으로 branch들을 나누어가기 때문에 근처의 데이터들도 전혀 다른 leaf로 구분되기도 한다.


## manifold learning
manifold는 머신러닝에서 중요한 개념 중 하나다.

minifold란 국소적으로는 Euclidean space와 위상학적으로 같은 연속된 점들의 집합이다.

우리는 일상적으로 평면 위에서 생활하지만, 지구는 사실상 굴곡진 3D라는 것과 직관적으로는 비슷한 개념이라고 보면 된다. 

수학적으로 이를 설명하는 방법이 있지만, 머신러닝에서는 대강 고차원 공간 상에서, 저차원으로 임베딩 된 연속된 점들의 집합공간 정도로 여겨진다.


- 매니폴드 그림

위 그림에 2차원 공간 상에서 1차원으로 존재하는 점들의 집합공간 manifold가 묘사되어 있다. 머신러닝에서는 manifold의 차원은 유동적이다. 예를 들어, 파란 선들의 교차점에서는 2차원 manifold로도 볼 수 있는 것이다. 

- 노이즈 그림

manifold hypothesis는 image, text, sount 등의 데이터드의 분포가 특정 영역(manifold)에 분포되어 있다는 가설이다. 


만약 이미지의 데이터 분포가 특정 영역에 몰려있지 않다면 uniform random을 통해 데이터를 뽑았을 때 그럴듯한 이미지가 나와야 하지만 실제로는 노이즈가 잔뜩 낀 이미지만 나온다고 한다.


이러한 data probability distribution을 학습하여 이용하는 딥러닝 연구들은 매우 많이 나와있다. 이전부터 많이 사용되던 오토인코더를 비롯해 최근에는 이미지에서 여러 가지 특성의 manifold를 분리해내는 disentabgled representaion learning도 활발하게 연구되고 있다.







# Reference
- [티스토리. 차원의 저주](https://bioinformaticsandme.tistory.com/197)
- [Velog. ch5. Machine Learning Basics](https://velog.io/@chayuhyun/Ch5.-Machine-Learning-Basics)
- [티스토리. Scaling Up Deep Learning - Yoshua Bengio](https://enginius.tistory.com/512)
- [wikipedia. Euclidean space](https://ko.wikipedia.org/wiki/%EC%9C%A0%ED%81%B4%EB%A6%AC%EB%93%9C_%EA%B3%B5%EA%B0%84)
