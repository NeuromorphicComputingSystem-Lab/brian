# Architecture Design
Architecture란 network의 전반적 구조를 의미합니다. 전반적 구조란 보통 유닛의 개수 설정, 유닛들의 상호 연결 방법을 의미합니다.

대부분은 neural network는 이전 레이어의 출력이 다음 레이어의 입력으로 들어가는 chain-based architecture를 이룹니다. 

Main architectural considerations in chain-based architecturs로는

- 네트워크의 깊이 결정하기
- 각 레이어별 너비 결정하기

와 같은 것들이 있습니다.

우리가 알고 있듯이 하나의 히든 레이어는 트레이닝 셋에 충분히 맞습니다. 그리고 깊은 네트워크는 보통

- 각 레이어당 더 적은 유닛
- 더 적은 파라미터
- 테스트 셋에 generalizaion이 잘 됩니다.

하지만 보통 최적화하기 어렵습니다.

따라서 작업에 적합한 네트워크 아키텍쳐는 validation set error에 따른 실험으로 찾아내야 합니다.

## universal approximaion properties and depth

feature와 output을 행렬곱으로 맵핑하는 linear 모델은 convex optimization을 사용해 해결할 수 있다.

(convex optimizaiton(볼록 최적화)이란?)

objective function이 convex function이고, feasible region C가 convex set일 때를 convex optimization이다. local한 솔루션이 global한 솔루션이 되는 게 장점.

(볼록 함수): 위로 볼록한 함수

(볼록 집합): 볼록함수에서 함수 위의 부분(함숫값보다 큰 값의 집합). euclidean space에 속하는 집합A에 대해, 그 안의 임의의 두 점을 골랐을 대 둘을 연결하는 선분이 A에 포함될 경우, A는 볼록 집합이다. 예를 들어 속이 찬 공은 볼록 집합이지만 안 쪽에 구멍이 있는 경우 볼록 집합이 아니다.

non-linear 모델 또한 hidden layer가 있는 feed forward network는 어떤 함수든 근사할 수 있기 때문에 표현이 가능합니다.

MLP가 함수를 표현할 수 있다고 해도 학습이 실패할 수 있습니다.

- 최적화 알고리즘이 학습해야 하는 함수에 적합하지 않은 경우
- 학습 알고리즘이 overfitting되어 엉뚱한 함수를 학습하는 경우

싱글 레이어 피드 포워드네트워크는 모든 기능을 표현하기에 충분합니다. 하지만 layer의 크키가 너무 크면 학습 및 일반화에 실패할 수 있습니다.

대부분의 상황에서, 더 깊이 모델을 사용하면 원하는 함수를 표현하는 데 필요한 parameter 수를 줄이고 generalizaiton error를 줄일 수 있습니다.

"no free lunch" theorem에서 언급했듯이 모든 것을 만족시키는 만능 AI는 없습니다. 

만능 근사 원리란 주어진 함수를 MLP를 이용해 표현가능하다는 것일 뿐, 학습한 함수가 test set에 대해서도 만능으로 잘 작동한다는 뜻은 아닙니다. 따라서 파라미터 수를 줄여가며 원하는 함수를 representation 해야 합니다.

그림(그림6.5)는 레이어를 깊게 쌓아감에 따라 decision boundary가 어떻게 변화하는지 보여줍니다.

activation function은 절댓값을 사용했습니다. 따라서 레이어 하나를 지날 때마다 반절씩 접는 것으로 표현되었습니다. 

affine transformation은 반절 접는 위치를 정해줍니다.

레이어를 지날수록 근사해야 하는 함수가 단순해지고, 따라서 generalization error가 줄어듭니다.

## Other architectural considerations

지금까지는 뉴럴넷을 단순히 레이어들의 연결로 보고, 레이어의 깊이와 너비(유닛수)에 대해서 이야기 했지만 다양한 뉴럴넷이 있고 합니다. 이 책에서는 이후에 convolution network, 또는 sequece data를 처리하기 위해 고안된 recurrent network에 대해 언급한다고 합니다.

뉴럴넷은 기본적으로 메인 체인을 설계하는 것이지만, 이 외에도 skip connection, attention, convolutional filter 등의 설계가 중요하다고 합니다.

- skip connection

i번째 레이어에서 i+1이 아닌 i+2번째 레이어로 바로 넘겨서 gradient flow를 용이하게 만드는 기법이라고 합니다.

- attention

representation의 일부만 선택적으로 넘겨서 효율을 향상시키는 방법이라고 합니다.

- convolutional filter

ocnvolution network 도메인 내에서 filter를 공유함으로써 적은 파라미터로도 복잡한 패턴을 효율적으로 잡아낼 수 있게 설계하는 방법이라고 합니다.

- (number of hidden layer, Test accuracy) 그래프

이 그래프를 보면 네트워크의 깊이 즉, 히든 레이어의 개수가 늘어남에 따라 test accuracy가 증가함을 볼 수 있습니다. 

deeper model 은 더 나은 성능을 보이는 경향이 있습니다. 이것은 단지 모델이 더 크기 때문만은 아닙니다. 그래프를 보면 깊이를 증가시키지 않고 컨볼루션 네트워크의 레이어에서 매개 변수를 늘리는 것이 test performance를 높이는 데 효과적이지 않다는 것을 보여줍니다.

legend는 각 curve를 만드는 데 사용되는 네트워크의 깊이와 curve가 convolutional 또는 fully connected의 레이어의 크기 변화를 나타내는지 여부를 나타냅니다.

그래프를 보면 얕은 모델이 약 2천만 개의 파라미터에 overfit 하는 반면 깊은 모델은 6천만 개 이상의 매개 변수를 갖는 것이 유리하다는 것을 보여줍니다. 이것은 깊은 모델을 사용할 때는 단순한 함수로 구성된 표현을 학습하는 것, 그리고 이 함수들을 순차적으로 종속적인 절차로 프로그램을 학습하는 것이 효과적이라고 볼 수 있습니다.